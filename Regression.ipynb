{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPChDVlbHxRozH+LZzgc7Qv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bugatha1/Regression/blob/main/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outliers\n",
        "Outlier is a datapoint which is different from other observations.              \n",
        "It indicates experimental error. Outliers can alter the performance many machine learning algorithms.                                                    \n",
        "How to deal with outliers?                                                      \n",
        "we can drop them alltogether, cap them with threshold, assign the new values with mean of the dataset for example or applying transformation in dataset itself.\n",
        "\n",
        "How to deal with missing data?                                                  \n",
        "one way to handle missing by dropping the observations or use data imputation techniques. if missing values or numeric values, we can replace with mean, median, mode or replacing with random sample values or replace with regression value.                                                                          \n",
        "if it is categorical value replace with mode or kNN prediction value."
      ],
      "metadata": {
        "id": "kdDJSiYZyT8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics suggest that, to measure the difference between the prediction and the real value, we should square the differences and then sum them all. This is called the squared sum of errors"
      ],
      "metadata": {
        "id": "jF9d1CyuDUBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine learning algorithms can learn in three ways"
      ],
      "metadata": {
        "id": "mAKfsC2_C1bM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Supervised learning\n",
        "2.   Unsupervised learning\n",
        "3.   Reinforcement learning"
      ],
      "metadata": {
        "id": "9qsSBSqaIw9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Supervised learning                                                        \n",
        "This is when we present labeled examples to learn from.                        \n",
        "Classification and regression are the examples for supervised learning "
      ],
      "metadata": {
        "id": "RmzECIriBdY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regression                                                                  \n",
        "Regression is used to identify relationship between a dependent variable and indenpendent variables.                                                         \n",
        "Forecasting and timeseries models coming under regression.\n",
        "\n",
        "1.   Linear regression\n",
        "2.   Decision tress\n",
        "3.   Random forest\n",
        "4.   SVM"
      ],
      "metadata": {
        "id": "mjnTnURU9fVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification\n",
        " Classification models are used to predict the target that has discreate values. \n",
        "\n",
        "1.   Logistic regression\n",
        "2.   kNN\n",
        "3.   Decision trees\n",
        "4.   SVM\n",
        "5.   Naive Bayes"
      ],
      "metadata": {
        "id": "YDmep4qY_wI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised learning                                                      \n",
        "This is when we present examples without any hint, leaving it to the algorithm to create a label.  It is mainly used for clustering and association tasks.\n",
        "\n",
        "*   Customer segmentation, or understanding different customer groups around which to build marketing or other business strategies.\n",
        "*   Recommender systems, which involve grouping together users with similar viewing patterns in order to recommend similar content.\n",
        "*   Anomaly detection, including fraud detection or detecting defective mechanical parts (i.e., predictive maintenance).\n",
        "\n",
        "Unsupervised learning has important applications in robotic vision and automatic feature creation"
      ],
      "metadata": {
        "id": "A7HDJdFCBn-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering & Dimensionality reduction\n",
        "\n",
        "*   PCA\n",
        "*   K-means"
      ],
      "metadata": {
        "id": "o8M914V9CGRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Association Analysis\n",
        "\n",
        "*   Apriori\n",
        "*   FP-Growth"
      ],
      "metadata": {
        "id": "uNhuTvjhCS_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Reinforcement learning   \n",
        "Agent learning by interacting with environment. Agent performs action in the environment, this action takes the environment to new state and gives the reward to agent. The reward can be negative or positive. Multiple iterations and rewards agents learns with his past experience. It mainly used in scaled acquisition tasks, like robot navigation and games.\n",
        "This is when we need software to act successfully in a competitive setting, such as a videogame or the stock market, we can use reinforcement learning. In this case, the software will then start acting in the setting and it will learn directly from its errors until it finds a set of rules that ensure its success."
      ],
      "metadata": {
        "id": "TFpCFz3mCsmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfitting\n",
        "If the model performs well on training dataset and not performing well on test dataset that model is called overfitted model.\n",
        "\n",
        "to fix this, gather more data. This is not always feasible. Need to use penality terms in the model(regularisation technique) or use the cross validation. \n",
        "\n",
        "# Underfitting\n",
        "when the training error is large because the model is so simple.\n",
        "\n",
        "To fix this, need to increase the size of the dataset or increase the complexity of the model."
      ],
      "metadata": {
        "id": "zzg8_XljD7tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias & Variance\n"
      ],
      "metadata": {
        "id": "0vBu4dtFNFk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models errors are classified into 2 categories\n",
        "\n",
        "1.   irreducible error : this will come from nature of the data (ex: noise is coming when we talk mobile phone)\n",
        "2.   reducible error\n",
        "      * bias error : is the differnce between the average prediction of our model and the correct value which we are trying to predict.\n",
        "      * variance error"
      ],
      "metadata": {
        "id": "v-j_pV52NSzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model\n",
        "It helps us to specify a linear relationship to predict the numberical value of a dependent variable(y) for given value of independent variables(X) bu using a straight line.                                                                  \n",
        "     y = b0 + bX + e\n",
        "\n",
        "     b0 is the intercept.\n",
        "     b is the coefficient associated to X.\n",
        "     e or error denotes all remaining information about y that hasn't been explained by X."
      ],
      "metadata": {
        "id": "QyGKTeEcFweW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "R015LOwPydYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicting the test set results"
      ],
      "metadata": {
        "id": "GefBF569zFU9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "1fQDiPW5zKu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualising the training test results "
      ],
      "metadata": {
        "id": "DdtNOeIQzwRQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_trainl, y_train, color='red')\n",
        "plt.plot(X_train, regressor.predict(X_train), color='blue')\n",
        "plt.title('...')\n",
        "plt.xlabel('...')\n",
        "plt.ylabel('...')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_s7QX8nHz5B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multiple linear regression model on training set"
      ],
      "metadata": {
        "id": "2zxSAjZCEvhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gDtC55ftFWcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial regression model"
      ],
      "metadata": {
        "id": "4XlWqjrxMTrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_reg = PolynomialFeatures(degree = 2)\n",
        "X_poly = poly_reg.fit_transform(X)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y) "
      ],
      "metadata": {
        "id": "Z-MxNYFtMaDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualise the polynomial regression results"
      ],
      "metadata": {
        "id": "rjhkHsvmN4bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X, y, color = 'red')\n",
        "plt.plot(X, lin_reg.predict(poly_reg.fit_transform(X)), color = 'blue')\n",
        "plt.title('...')\n",
        "plt.xlabel('...')\n",
        "plt.ylabel('...')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CdSigfDGN-lj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support vector regression\n",
        "  we have support vector linear regression and support vector nonlinear regression"
      ],
      "metadata": {
        "id": "SO_i7NRXk195"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVR model on the whole dataset\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "regressor = SVR(kernel = 'rbf')\n",
        "regressor.fit(X, y)"
      ],
      "metadata": {
        "id": "L4tQj3fCk8VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees\n",
        " Classification Trees and Regression Trees"
      ],
      "metadata": {
        "id": "-pn06xuhunjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the decision tree regression model on whole dataset\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "regressor = DecisionTreeRegressor(random_state=0)\n",
        "regressor.fit(X, y)"
      ],
      "metadata": {
        "id": "Vw7mI7gK4mB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest\n",
        "\n",
        "It is a version of ensemble learning. Ensemble learning means we are taking same alogirthm muliple times and put them together to make it powerful than original one. \n",
        "\n",
        "step1 :  Pick at random K data points from the training set.                    \n",
        "step2 : Build the decision tree associated to these K data points.              \n",
        "step3 : Chooes the number Ntree of trees you want to build and repeat STEPS 1&2\n",
        "step4 : For a new data point, make each one of your Ntree trees predict the value of Y to fot the data point in question, and assign the new data point the average across all of the predicted Y values "
      ],
      "metadata": {
        "id": "O_6xj7Sw5c5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the random forest regression model on the whole dataset\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regressor = RandomForestRegressor(n_estimators=10, random_state=0)\n",
        "regressor.fit(X, y)"
      ],
      "metadata": {
        "id": "Gzo-T9h0FXXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Python packages and functions for linear models\n",
        "\n",
        "**NumPy**                                                                           \n",
        "it is at the core of every analytical solution in the Python language. It provides the user with multidimensional arrays, along with a large set of functions to operate multiple mathematical operations on the arrays.\n",
        "\n",
        "**SciPy**                                                                       \n",
        "It completes NumPy's functionalities, offering a larger variety of scientific algorithms for linear algebra, sparse matrices, signal and image processing, optimization, fast Fourier transformation, and much more.\n",
        "\n",
        "**Scikit-learn**                                                                \n",
        "It is part of the SciPy Toolkits (SciKits), Scikit-learn is the core of data science operations on Python.                                                   \n",
        "Scikit-learn offers modules for data processing (sklearn.preprocessing, sklearn.feature_extraction), model selection, and validation (sklearn.cross_validation, sklearn.grid_search, and sklearn.metrics) and a complete set of methods (sklearn.linear_model)"
      ],
      "metadata": {
        "id": "a1_tarAL2Kwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vector = np.array([1, 2, 3, 4, 5])\n",
        "#vector\n",
        "\n",
        "row_vector = vector.reshape((5,1))\n",
        "#row_vector\n",
        "\n",
        "column_vector = vector.reshape((1, 5))\n",
        "#column_vector\n",
        "\n",
        "single_feature_matrix = vector.reshape((1, 5))\n",
        "#single_feature_matrix\n",
        "\n",
        "all_zeros = np.zeros((2,5))\n",
        "#all_zeros\n",
        "\n",
        "all_ones = np.ones((3,4))\n",
        "all_ones"
      ],
      "metadata": {
        "id": "ielpXG1d-K3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "boston\n",
        "california = fetch_california_housing()\n",
        "california"
      ],
      "metadata": {
        "id": "SVG4HmJU-POG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "r08WjKko-Twe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "dataset['target'] = california.target\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "FCn6rLSI-X9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "dataset['target'] = boston.target\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "JetKuzj6-bc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "\n",
        "x = np.linspace(-4, 4, 100)\n",
        "for mean, variance in [(0,0.7),(0,1),(1,1.5),(-2,0.5)]:\n",
        "  plt.plot(x, norm.pdf(x, mean, variance))\n",
        "plt.show"
      ],
      "metadata": {
        "id": "qT6tywkO-e-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_expected_value = dataset['target'].mean()\n",
        "mean_expected_value"
      ],
      "metadata": {
        "id": "uERgal4K-kfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['target'].sum())\n",
        "print(np.sum(dataset['target']))"
      ],
      "metadata": {
        "id": "dbEFSmvy-n1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Squared_errors = pd.Series(mean_expected_value - dataset['target'])**2\n",
        "SSE = np.sum(Squared_errors)\n",
        "print('Sum of squared errors (SSE): %01.f' %SSE)"
      ],
      "metadata": {
        "id": "lcYV5Ue5-r1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "density_plot = Squared_errors.plot(kind = 'hist')"
      ],
      "metadata": {
        "id": "YC32pfnd-vzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}