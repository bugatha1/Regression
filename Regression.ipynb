{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyORq3/jHBMdxdBJf7RWGqbJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bugatha1/Regression/blob/main/Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Outliers\n",
        "Outlier is a datapoint which is different from other observations.              \n",
        "It indicates experimental error. Outliers can alter the performance in many machine learning algorithms.                                                    \n",
        "How to deal with outliers?                                                      \n",
        "we can drop them alltogether, cap them with threshold, assign the new values with mean of the dataset for example or applying transformation in dataset itself.\n",
        "\n",
        "How to deal with missing data?                                                  \n",
        "one way to handle missing by dropping the observations or use data imputation techniques. if missing values or numeric values, we can replace with mean, median, mode or replacing with random sample values or replace with regression value.                                                                          \n",
        "if it is categorical value replace with mode or kNN prediction value."
      ],
      "metadata": {
        "id": "kdDJSiYZyT8Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics suggest that, to measure the difference between the prediction and the real value, we should square the differences and then sum them all. This is called the squared sum of errors"
      ],
      "metadata": {
        "id": "jF9d1CyuDUBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Machine learning algorithms can learn in three ways"
      ],
      "metadata": {
        "id": "mAKfsC2_C1bM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Supervised learning\n",
        "2.   Unsupervised learning\n",
        "3.   Reinforcement learning"
      ],
      "metadata": {
        "id": "9qsSBSqaIw9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Supervised learning                                                        \n",
        "This is when we present labeled examples to learn from.                        \n",
        "Classification and regression are the examples for supervised learning "
      ],
      "metadata": {
        "id": "RmzECIriBdY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression**                                                                  \n",
        "Regression is used to identify relationship between a dependent variable and indenpendent variables.                                                         \n",
        "Forecasting and timeseries models coming under regression.\n",
        "\n",
        "1.   Linear regression\n",
        "2.   Decision tress\n",
        "3.   Random forest\n",
        "4.   SVM"
      ],
      "metadata": {
        "id": "mjnTnURU9fVF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Classification**                                                              \n",
        "Classification models are used to predict the target that has discreate values. \n",
        "\n",
        "1.   Logistic regression\n",
        "2.   kNN\n",
        "3.   Decision trees\n",
        "4.   SVM\n",
        "5.   Naive Bayes"
      ],
      "metadata": {
        "id": "YDmep4qY_wI6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Unsupervised learning                                                      \n",
        "This is when we present examples without any hint, leaving it to the algorithm to create a label.  It is mainly used for clustering and association tasks.\n",
        "\n",
        "*   Customer segmentation, or understanding different customer groups around which to build marketing or other business strategies.\n",
        "*   Recommender systems, which involve grouping together users with similar viewing patterns in order to recommend similar content.\n",
        "*   Anomaly detection, including fraud detection or detecting defective mechanical parts (i.e., predictive maintenance).\n",
        "\n",
        "Unsupervised learning has important applications in robotic vision and automatic feature creation"
      ],
      "metadata": {
        "id": "A7HDJdFCBn-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering & Dimensionality reduction**\n",
        "\n",
        "*   PCA\n",
        "*   K-means"
      ],
      "metadata": {
        "id": "o8M914V9CGRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Association Analysis**\n",
        "\n",
        "*   Apriori\n",
        "*   FP-Growth"
      ],
      "metadata": {
        "id": "uNhuTvjhCS_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Reinforcement learning   \n",
        "Agent learning by interacting with environment. Agent performs action in the environment, this action takes the environment to new state and gives the reward to agent. The reward can be negative or positive. Multiple iterations and rewards agents learns with his past experience. It mainly used in scaled acquisition tasks, like robot navigation and games.\n",
        "This is when we need software to act successfully in a competitive setting, such as a videogame or the stock market, we can use reinforcement learning. In this case, the software will then start acting in the setting and it will learn directly from its errors until it finds a set of rules that ensure its success."
      ],
      "metadata": {
        "id": "TFpCFz3mCsmM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overfitting\n",
        "If the model performs well on training dataset and not performing well on test dataset that model is called overfitted model.\n",
        "\n",
        "to fix this, gather more data. This is not always feasible. Need to use penality terms in the model(regularisation technique) or use the cross validation. \n",
        "\n",
        "# Underfitting\n",
        "when the training error is large because the model is so simple.\n",
        "\n",
        "To fix this, need to increase the size of the dataset or increase the complexity of the model."
      ],
      "metadata": {
        "id": "zzg8_XljD7tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias & Variance\n"
      ],
      "metadata": {
        "id": "0vBu4dtFNFk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models errors are classified into 2 categories\n",
        "\n",
        "1.   irreducible error : this will come from nature of the data (ex: noise is coming when we talk mobile phone)\n",
        "2.   reducible error\n",
        "      * bias error : is the differnce between the average prediction of our model and the correct value which we are trying to predict.\n",
        "      * variance error"
      ],
      "metadata": {
        "id": "v-j_pV52NSzM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Model\n",
        "It helps us to specify a linear relationship to predict the numberical value of a dependent variable(y) for given value of independent variables(X) by using a straight line.  \n",
        "\n",
        "     y = b0 + bX + e\n",
        "\n",
        "     b0 is the intercept.\n",
        "     b is the coefficient associated to X.\n",
        "     e or error denotes all remaining information about y that hasn't been explained by X."
      ],
      "metadata": {
        "id": "QyGKTeEcFweW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Linear Regression Model**\n",
        "\n",
        "Using single independent variable to predict dependent variable.                \n",
        "\n",
        "y = b0 + b1x1 + e"
      ],
      "metadata": {
        "id": "Y_hs08gZHJjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "plt.scatter(X_trainl, y_train, color='red')\n",
        "plt.plot(X_train, regressor.predict(X_train), color='blue')\n",
        "plt.title('...')\n",
        "plt.xlabel('...')\n",
        "plt.ylabel('...')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R015LOwPydYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Multiple Linear Regression Model**\n",
        "\n",
        "Using two or more independent variables to predict dependent variable.          \n",
        "\n",
        "y = b0 + b1x1 + b2x2 + .... + e"
      ],
      "metadata": {
        "id": "2zxSAjZCEvhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "regressor = LinearRegression()\n",
        "regressor.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "gDtC55ftFWcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Estimation of coefficients**\n",
        "\n",
        "We use the least squares or the sum of the individual squared errors.\n",
        " error of data point i = yi - yi^\n",
        " sum of the squared errors = Sum of i=1 to n e1**2 "
      ],
      "metadata": {
        "id": "co4qI3jdJIO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial regression model"
      ],
      "metadata": {
        "id": "4XlWqjrxMTrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "poly_reg = PolynomialFeatures(degree = 2)\n",
        "X_poly = poly_reg.fit_transform(X)\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_poly, y) \n",
        "\n",
        "\n",
        "plt.scatter(X, y, color = 'red')\n",
        "plt.plot(X, lin_reg.predict(poly_reg.fit_transform(X)), color = 'blue')\n",
        "plt.title('...')\n",
        "plt.xlabel('...')\n",
        "plt.ylabel('...')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z-MxNYFtMaDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support vector regression\n",
        "  we have support vector linear regression and support vector nonlinear regression"
      ],
      "metadata": {
        "id": "SO_i7NRXk195"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVR model on the whole dataset\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "regressor = SVR(kernel = 'rbf')\n",
        "regressor.fit(X, y)"
      ],
      "metadata": {
        "id": "L4tQj3fCk8VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Trees\n",
        "\n",
        "It can be used for categorical and regression usecases.                         \n",
        "It is a series fo if-else conditions that lead to a decision.\n",
        "It has 3 kind of nodes.                                                         \n",
        "\n",
        "A decision node has two or more branches.                                       \n",
        "A leaf node represents a classification or decision.\n",
        "A topmost decision node corrsponds to the best predictor called root node.\n",
        "\n",
        "3 important steps to create decision tree\n",
        "1. **Splitting**: \n",
        "    Process of creating sub branch is called splitting. The root node has full dataset, each decision node the test is conducted to split dataset. Decision nodes are executed to split the data until it will reach to the leaf node. A leaf node contains single target value.\n",
        "\n",
        "2. **Pruning** : (shortening of branches of the tree)                           \n",
        "      preprunning: stopping the creation of tree (limitting the maximum depth of the tree)\n",
        "      postprunning: trimming the nodes that have less information.\n",
        "3. **Tree selection** : \n",
        "      A prediction made by traversing through the decision tree and checking the each decision node whether the condition is met or not.\n",
        "\n",
        "\n",
        "**Hyper parameters**\n",
        "\n",
        "1. The maximum depth of the tree\n",
        "2. The maximum number of the leaf nodes\n",
        "3. The minimum number of the data points required to split the node further.\n",
        "\n",
        "**Pros & Cons**\n",
        "\n",
        "computationanlly cheap to use, easy for humans undetstand results\n",
        "\n",
        "prone to overfitting and provide poor generalization performance."
      ],
      "metadata": {
        "id": "-pn06xuhunjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the decision tree regression model on whole dataset\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "regressor = DecisionTreeRegressor(random_state=0)\n",
        "regressor.fit(X, y)"
      ],
      "metadata": {
        "id": "Vw7mI7gK4mB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entropy\n",
        "\n",
        "one of the key factor in decision tree is Entropy.\n",
        "It is important to consider which feature will be used to split each node.\n",
        "we can utilize a statical method to select the feature that has the most information gain as the root node.\n",
        "Information gain is measured in terms of the expected reduction in the entropy or impurity of the data\n",
        "The entropy of a set of of probabilities is H of P in the formula.              \n",
        "\n",
        "A training dataset has 500 observations. 300 are positive and 200 are negative.\n",
        "positive class ratio = 300/500 = 0.6\n",
        "negative class ratio = 200/500 = 0.4\n",
        "\n",
        "Entropy = -0.6 * log2(0.6) + 0.4 log2(0.4) = 0.9702\n",
        "\n",
        "X > 347 (120 positive and 80 negative), X <=347 (240 positive and 60 negative)\n",
        "\n",
        "Entropy of X > 347 = E1 = -120/200 log2(120/200) - 80/200 log2(80/200)\n",
        "Entropy of X <= 347 = E2 = -240/300 log2(240/300) - 60/300 log2(60/300)\n",
        "Entropy of X = 200/500 * Entropy1 + 300/500 * Entropy2 \n",
        "\n",
        "Information gain for feature X = Entropy total - Entropy of X"
      ],
      "metadata": {
        "id": "vQT8StiLoUKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ensemble\n",
        "\n",
        "This technique is used to average the results to reduce the overfitting\n",
        "Ensemble means assembling many machine learing models to create more robust model to reduce overfitting\n",
        "\n",
        "most popular Ensemble models are                                               \n",
        "\n",
        "1. **Bagging** : It combines sampling techniques and aggregation to form an ensemble model. In practice, multiple samples are choose randomly with replacement within the trainig dataset. ex : sample of 10 observations drawan from traing dataset of 100 observations. This observations return to the dataset before another set is drawn. for each of the samples decision tree or other baselines are creating. Finally these baselines or decision trees are aggregate to achieve efficient predictor. The output of final prediction choosen by voting for classification and averaging for regression.              \n",
        "\n",
        "2. **RandomForest** : During bagging all features of the trainging dataset are used on sample data to create the decision trees or base estimators. In random forest the samples are created with a subset of features selected randomly for each node in the decision tree.\n",
        "\n",
        "Ensemble model hyper parameters\n",
        "Nubmer of decision tress\n",
        "Maximum number of features\n",
        "Maximum depth of the decision tree\n",
        "Minimum number of samples required at each leaf node\n",
        "Minimum number of samples required to split a node\n",
        "Maximum number of leaf nodes in each decision tree"
      ],
      "metadata": {
        "id": "v44a85OSFgM3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest\n",
        "\n",
        "It is a version of ensemble learning. Ensemble learning means we are taking same alogirthm muliple times and put them together to make it powerful than original one. \n",
        "\n",
        "step1 :  Pick at random K data points from the training set.                    \n",
        "step2 : Build the decision tree associated to these K data points.              \n",
        "step3 : Chooes the number Ntree of trees you want to build and repeat STEPS 1&2\n",
        "step4 : For a new data point, make each one of your Ntree trees predict the value of Y to fot the data point in question, and assign the new data point the average across all of the predicted Y values "
      ],
      "metadata": {
        "id": "O_6xj7Sw5c5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the random forest regression model on the whole dataset\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regressor = RandomForestRegressor(n_estimators=10, random_state=0)\n",
        "regressor.fit(X, y)"
      ],
      "metadata": {
        "id": "Gzo-T9h0FXXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Boosting\n",
        "It incrementally builds an essemble by training each model with the same dataset but where the model coefficients of estimators are adjusted according to the error of the last prediction.                                            \n",
        "The main idea of boosting is to focus on the observations that are hard to predict. Boosting can reduce bias without incurring higher variance.            \n",
        "\n",
        "two boosting algorithms\n",
        "\n",
        "**AdaBoost**\n",
        "  it is adaptive boosting, where more attention is given to the records that are not correctly predicted.                                                    \n",
        "\n",
        "**Gradient Boosting**                                                           \n",
        "It works by sequentially adding the previous predictor's underfitted predictions to the ensemble, ensuring errors made previously corrected.\n",
        "\n",
        "Hyperparameters\n",
        "The number of decision trees\n",
        "Maximum depth (usually giving 5)\n",
        "Learning rate\n"
      ],
      "metadata": {
        "id": "vPzF-A-wn_tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Python packages and functions for linear models\n",
        "\n",
        "**NumPy**                                                                           \n",
        "it is at the core of every analytical solution in the Python language. It provides the user with multidimensional arrays, along with a large set of functions to operate multiple mathematical operations on the arrays.\n",
        "\n",
        "**SciPy**                                                                       \n",
        "It completes NumPy's functionalities, offering a larger variety of scientific algorithms for linear algebra, sparse matrices, signal and image processing, optimization, fast Fourier transformation, and much more.\n",
        "\n",
        "**Scikit-learn**                                                                \n",
        "It is part of the SciPy Toolkits (SciKits), Scikit-learn is the core of data science operations on Python.                                                   \n",
        "Scikit-learn offers modules for data processing (sklearn.preprocessing, sklearn.feature_extraction), model selection, and validation (sklearn.cross_validation, sklearn.grid_search, and sklearn.metrics) and a complete set of methods (sklearn.linear_model)"
      ],
      "metadata": {
        "id": "a1_tarAL2Kwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "vector = np.array([1, 2, 3, 4, 5])\n",
        "#vector\n",
        "\n",
        "row_vector = vector.reshape((5,1))\n",
        "#row_vector\n",
        "\n",
        "column_vector = vector.reshape((1, 5))\n",
        "#column_vector\n",
        "\n",
        "single_feature_matrix = vector.reshape((1, 5))\n",
        "#single_feature_matrix\n",
        "\n",
        "all_zeros = np.zeros((2,5))\n",
        "#all_zeros\n",
        "\n",
        "all_ones = np.ones((3,4))\n",
        "all_ones"
      ],
      "metadata": {
        "id": "ielpXG1d-K3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.datasets import load_boston\n",
        "\n",
        "boston = load_boston()\n",
        "boston\n",
        "california = fetch_california_housing()\n",
        "california"
      ],
      "metadata": {
        "id": "SVG4HmJU-POG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl"
      ],
      "metadata": {
        "id": "r08WjKko-Twe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame(california.data, columns=california.feature_names)\n",
        "dataset['target'] = california.target\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "FCn6rLSI-X9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "dataset['target'] = boston.target\n",
        "dataset.head()"
      ],
      "metadata": {
        "id": "JetKuzj6-bc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.mlab as mlab\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import norm\n",
        "\n",
        "x = np.linspace(-4, 4, 100)\n",
        "for mean, variance in [(0,0.7),(0,1),(1,1.5),(-2,0.5)]:\n",
        "  plt.plot(x, norm.pdf(x, mean, variance))\n",
        "plt.show"
      ],
      "metadata": {
        "id": "qT6tywkO-e-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_expected_value = dataset['target'].mean()\n",
        "mean_expected_value"
      ],
      "metadata": {
        "id": "uERgal4K-kfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset['target'].sum())\n",
        "print(np.sum(dataset['target']))"
      ],
      "metadata": {
        "id": "dbEFSmvy-n1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Squared_errors = pd.Series(mean_expected_value - dataset['target'])**2\n",
        "SSE = np.sum(Squared_errors)\n",
        "print('Sum of squared errors (SSE): %01.f' %SSE)"
      ],
      "metadata": {
        "id": "lcYV5Ue5-r1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "density_plot = Squared_errors.plot(kind = 'hist')"
      ],
      "metadata": {
        "id": "YC32pfnd-vzP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}